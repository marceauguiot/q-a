{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Q&A.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marceauguiot/q-a/blob/master/NLP_Q%26A_without_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdR5xw6CGPcN",
        "colab_type": "text"
      },
      "source": [
        "#**NLP Project : Q & A system**\n",
        "\n",
        "> Group 6 : Maud Galmiche, Thibaut Nicot, Christophe Arendt, Sebastien Bordonnat, Marceau Guiot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYmExvbdFvaM",
        "colab_type": "text"
      },
      "source": [
        "![NLP Project : Q & A system](https://www.escpeurope.eu/sites/default/files/default_images/default-picture-news-md.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1MPrQNYDvge",
        "colab_type": "text"
      },
      "source": [
        "In this NLP project, we are asked to predict the best answer if any exists. In the dataset provided, there is not always a best answer for each question. Another issue is that there are some comments upon some answers. In a first time, we will build a model only upon answers. \n",
        "\n",
        "For this project, we decided to work on google colab. In fact as students, we did not have access to a GPU with our laptop so we decided to use the one freely provided by google colab. To colaborate, we decided to write all our code on a notebook. Since this project has non intent to be put in production, we think that would be the most convenient format to work on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OExlCX-AMBc2",
        "colab_type": "text"
      },
      "source": [
        "This project will be strucutred in 3 main parts : \n",
        "\n",
        "\n",
        "1.   Data preprocessing\n",
        "2.   Data vizualization\n",
        "3.   Model training using Flair\n",
        "\n",
        "In the data preprocessing, we will discuss a little bit about the data and how we can import them. Quality of data is often a big deal for data scientists. In the data provided by the dataset, some separators can be misunderstood by regular fonctions normally used to import csv as the one provided by pandas. That is why, in this part, we will try to figure out how we can import the data provided.\n",
        "\n",
        "In data vizualization, we will try to figure out how our data are strctured. We will also have a close look on how our labels are balanced ( the number of best answers compared to the total amount of answers). We will also give close attention to the length of each answer since it can cause some memory issue when we are doing the embedding. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37mE9rsoHBZm",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWzd4Gkiy9CC",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jocV6iZwfGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General libraries\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import torch\n",
        "import string\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from hyperopt import hp\n",
        "from tqdm import tqdm \n",
        "import cufflinks as cf\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# From scikit learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "#From nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "\n",
        "\n",
        "#Google colab\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2mvm4YhBn99",
        "colab_type": "text"
      },
      "source": [
        "## Importing drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFGJlcphVsyW",
        "colab_type": "text"
      },
      "source": [
        "In order to have access to the dataset provided, we mount our drive containing the csv file. This operation allow us to import files from the drive but also to write and save files on the google drive. We just have to copy the authorization code to access the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMA7-HdYmtrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmfEt4DFWRQ_",
        "colab_type": "text"
      },
      "source": [
        "Once the drive is mounted, we can change our working directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRMuKpsInB6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change working directory\n",
        "%cd \"/content/drive/My Drive/Q & A/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb_EWt4uM1IT",
        "colab_type": "text"
      },
      "source": [
        "##Importing data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgNCHb3_WZ_I",
        "colab_type": "text"
      },
      "source": [
        "As explained before, the data export-forum_en.csv are some format issues. In spite of this issues, we try to force the importation using pandas to have a first glance at the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKIVhSXQSkj-",
        "colab_type": "text"
      },
      "source": [
        "### With pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fCXc2pFzMxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"id\",\n",
        "\"question_answer_or_comment\" , \n",
        "\"is_best_answer\",\n",
        "\"topic_id\",\n",
        "\"parent_id\",\n",
        "\"votes\",\n",
        "\"titre\",\n",
        "\"message\",\n",
        "\"member\", \n",
        "\"category\", \n",
        "\"state\",\n",
        " \"is_solved\", \n",
        "\"num_answers\",\n",
        "\"country\", \n",
        "\"date\",\n",
        "\"last_answer_date\", \n",
        "\"auteur_crc\", \n",
        "\"visits\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74mnqm4Yv5eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"export-forums_en.csv\", error_bad_lines = False, names = columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_yISop7yP54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyy-HJgpYLjW",
        "colab_type": "text"
      },
      "source": [
        "Even if at first sight, the importation seems to have worked, we can see after a quick analysis that we have facing some issues. In fact, after a few lines, the importation process shifts all the columns, damaging the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJBdXOXhX_LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[data[\"is_best_answer\"] == ' update data according to official site'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4wsGhG1Xgb2",
        "colab_type": "text"
      },
      "source": [
        "### With a customized method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbp5PI3_aQlQ",
        "colab_type": "text"
      },
      "source": [
        "In a first time, we decided to work with the pandas importation excluding the data corrupted with a simple filtered. Since, our NLP professor M. Anh-Phuong decided to give us another method to import the whole dataset, we switched to his method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T73JL3pXoPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_path = \"export-forums_en.csv\"\n",
        "entity_path = \"export-forums_en.pickle\"\n",
        "csv_path = \"export-forums_en_TA.csv\"\n",
        "data_path = \"\"\n",
        "\n",
        "\n",
        "def format_entities():\n",
        "    '''\n",
        "    Read the raw data, format the list of entities, serialize them.\n",
        "    '''\n",
        "\n",
        "    def build_entities(txt_path, max_entities=None):\n",
        "        '''\n",
        "        Return a list of structured entities from raw txt file.\n",
        "        '''\n",
        "        # Read text file.\n",
        "        with open(txt_path, 'r', encoding='utf8') as f:\n",
        "            # Entities and current entity.\n",
        "            entities, entity = [], {}\n",
        "            # Entity values might be split over lines\n",
        "            field_counter = 0\n",
        "            # Process lines\n",
        "            for line in f:\n",
        "                # Prepare line\n",
        "                line = line.replace(\"\\\\N\", '\"unkwown\"')\n",
        "                # Char start for extracted value.\n",
        "                char_start = 1\n",
        "                # Find values separators\n",
        "                field_index = [m.start() for m in re.finditer('\",\"', line)]\n",
        "                # Browse value separators.\n",
        "                for index in field_index:\n",
        "                    # Extract in between value.\n",
        "                    value = line[char_start:index]\n",
        "                    # Update start index.\n",
        "                    char_start = index + 3\n",
        "                    # Update field counter.\n",
        "                    field_counter += 1\n",
        "                    # Update entity value.\n",
        "                    try:\n",
        "                        entity[columns[field_counter-1]] += value\n",
        "                    except KeyError:\n",
        "                        entity[columns[field_counter-1]] = value\n",
        "                    except IndexError:\n",
        "                        entity = {}\n",
        "                        field_counter = 0\n",
        "                # Content string is split.\n",
        "                if field_counter == 7 and len(field_index) > 0:\n",
        "                    entity[columns[7]] = line[field_index[-1]:]\n",
        "                    continue\n",
        "                # Next content string.\n",
        "                if field_counter == 7 and len(field_index) == 0:\n",
        "                    entity[columns[7]] += line\n",
        "                    continue\n",
        "                # Next entity.\n",
        "                if len(entity) == 17:\n",
        "                    field_counter = 0\n",
        "                    entities.append(entity)\n",
        "                    entity = {}\n",
        "                    if max_entities is not None:\n",
        "                        if len(entities) > max_entities:\n",
        "                            return entities\n",
        "        return entities\n",
        "\n",
        "    # Write entities on disk.\n",
        "    with open(entity_path, 'wb') as f:\n",
        "        pickle.dump(build_entities(txt_path=txt_path, max_entities=None), f)\n",
        "\n",
        "\n",
        "def entities_to_csv():\n",
        "    '''\n",
        "    Format entities to csv.\n",
        "    '''\n",
        "    with open(entity_path, 'rb') as obj:\n",
        "        entities = pickle.load(obj)\n",
        "    x = pd.DataFrame(entities, index = None)\n",
        "    x.to_csv(csv_path, index = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz4gS77SlZ-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "format_entities()\n",
        "entities_to_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omzb-4SubVaN",
        "colab_type": "text"
      },
      "source": [
        "### Checking imported data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfMYwc4KbYyA",
        "colab_type": "text"
      },
      "source": [
        "Once the csv is saved, we can easily checked that everything went well transforming our target variable into a set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrWa8EcdbXbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"export-forums_en_TA.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Cne5slgi1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC-w76spctFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set(data[\"is_best_answer\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpSKeAsZdArU",
        "colab_type": "text"
      },
      "source": [
        "Even if everything seems to went well during the importation, we are still facing a few issues. In fact the two columns date and last_answer_date are in second elapsed since the 1st January 1970. In order to have an appropriate view of this data we transform these two columns in proper date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgsvO3AF4yLl",
        "colab_type": "text"
      },
      "source": [
        "##Convert time to date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRPx_aU_eYnS",
        "colab_type": "text"
      },
      "source": [
        "In order to know in which fomat the time is represented we search on the web the forum related to the first question :  [click-here](https://ccm.net/forum/affich-4-windows-vista-to-xp-downgrading-reformat).  Once this is done, we just have to be sure that every columns is in the right format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo8lS6s-2lIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date = datetime(1970,1,1) # January 1st, 1904 at midnight\n",
        "\n",
        "delta = timedelta(seconds = 1207882212)\n",
        "\n",
        "newdate = date + delta\n",
        "print(newdate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7l9EMyT6BdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_date(seconds):\n",
        "  date = datetime(1970,1,1) # January 1st, 1904 at midnight\n",
        "\n",
        "  delta = timedelta(seconds = seconds)\n",
        "\n",
        "  newdate = date + delta\n",
        "  return(newdate)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhqR5bQjEVZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking the format of the both columns\n",
        "data['date'] = pd.to_numeric(data['date'], errors='coerce')\n",
        "data['last_answer_date'] = pd.to_numeric(data['last_answer_date'], errors='coerce')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuH2EzzH7RUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['date'] = data['date'].apply(lambda x: new_date(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFiQ0QPHIrJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['last_answer_date'] = data['last_answer_date'].apply(lambda x: new_date(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1e1IqmIlEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCyqj_NBnoxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['titre'] = data['titre'].apply(lambda x: str(x))\n",
        "data['message'] = data['message'].apply(lambda x: str(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU5acUXf57W0",
        "colab_type": "text"
      },
      "source": [
        "## Removing comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrLKBPC4hLPG",
        "colab_type": "text"
      },
      "source": [
        "As explained before, we start by building a model without considering the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MID242tj5-YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[ data.question_answer_or_comment != \"C\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy7LvJ-H6YEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOE__ByB6lIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a = data[ [\"titre\" ,\"message\", \"is_best_answer\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHh15qaS58ZM",
        "colab_type": "text"
      },
      "source": [
        "##Text cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7FkY26wKPiC",
        "colab_type": "text"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22azEYvriAy1",
        "colab_type": "text"
      },
      "source": [
        "One major issue we are facing with the dataset provided is the fact that this one is multilingual. This cause trouble for both cleaning text and the embedding. In fact, the library *nltk* provided some stopwords to clean the text but those are specific to one language. What can be done here is to build a model to detect the language of each question. Another soltuion, less time consuming is to concatenate the list of stopwords for each language. This [Git-hub](https://github.com/6/stopwords-json) provides a json file gathering stopwords from multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzOTr_0IKO_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/6/stopwords-json.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boS-RkhzKtQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('stopwords-json/stopwords-all.json') as json_file:  \n",
        "    stop_words = json.load(json_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VrU1dhKjct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"the\" in stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yqrbShrRGYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = list(stop_words.values())\n",
        "stop_words = set([item for sublist in stop_words for item in sublist])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaUmHWeTKmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"the\" in set(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHbRE8_qvOET",
        "colab_type": "text"
      },
      "source": [
        "Here we are using a set in stead of a list to decrease the computing time. Since we have now defined our stopwords we can define a function to clean the text. First we have to choose the variable on which we will apply the function.\n",
        "\n",
        "Since we want the information on both the question and the message, we decide here to concatenate the both columns. Once this will be done, we will be able to get rid off the titre of each question since it will also be contained in the message.\n",
        "\n",
        "The data provided might certainly come from scrapping. That is why we need to get clean all the HTML tags first. Once this is done we can replace all the punctuations characters with spaces and convert all the words to lower case. At theend we can apply our tokenizer on the text. Here we decided to use the *TreebankWordTokenizer*. At the end of the day, we check of course if the word is a stopowrd before adding it to our message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwj1Rhhn82Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Applies some pre-processing on the given text.\n",
        "\n",
        "    Steps :\n",
        "    - Removing HTML tags\n",
        "    - Removing punctuation\n",
        "    - Lowering text\n",
        "    \"\"\"\n",
        "    # remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # remove the characters [\\], ['] and [\"]\n",
        "    text = re.sub(r\"\\\\\", \"\", text)    \n",
        "    text = re.sub(r\"\\'\", \"\", text)    \n",
        "    text = re.sub(r\"\\\"\", \"\", text)\n",
        "    text = re.sub(r\"br\", \"\", text)\n",
        "    text = re.sub(r\"quot\", \"\", text)\n",
        "    text = re.sub(r\"http\", \"\", text)\n",
        "    \n",
        "    # remove other characters\n",
        "    text = re.sub('^(.*http)',\"\", text)\n",
        "    \n",
        "    # convert text to lowercase\n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    # replace punctuation characters with spaces\n",
        "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((c, \" \") for c in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    #tokenize\n",
        "    token = tokenizer.tokenize(text)\n",
        "    \n",
        "    # filter out stop words\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    words = [w for w in token if not w in stop_words]\n",
        "    words = \" \".join(words)\n",
        "\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9ndteWwhhG",
        "colab_type": "text"
      },
      "source": [
        "As explained before, we start with concatenating the both columns message and titre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZDXs9I-fzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tqdm.pandas()\n",
        "data_a['message'] = data_a[['titre', 'message']].apply(lambda x: ''.join(x), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ekzjWuQwqJF",
        "colab_type": "text"
      },
      "source": [
        "Once the concatenation is done, we can apply our function to clean the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m1rd6BlY-pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a['titre'] = data_a['titre'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdW1_5WEIjDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a['message'] = data_a['message'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TATGxj9QI46u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN1SZSfQaAY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a = data_a[[\"message\", \"is_best_answer\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8b0k1qh4Ewz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_a.to_csv(\"data_cleaned.csv\", index = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-hw4IkBXAVE",
        "colab_type": "text"
      },
      "source": [
        "# Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwDe0FfYp18h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"data_cleaned.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA2q2cjXqqOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy_VvWGdw8Cc",
        "colab_type": "text"
      },
      "source": [
        "One of the most important thing to do here is to check how balanced are our label in the target variables. Of course, as we have expected, the best answers are really scarce in our dataset. We have to keep that in our mind when will analyse the results of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mh7fbpwY_s-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
        "  init_notebook_mode(connected=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "482rZmoa-7ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enable_plotly_in_cell()\n",
        "fig = data['is_best_answer'].iplot(\n",
        "    kind='hist',\n",
        "    xTitle='rating',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    title='Review best answers Distribution',\n",
        "     )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwYTYGeDxYIG",
        "colab_type": "text"
      },
      "source": [
        "In order to see which are the most common words, we build a bag of words. Once this bag is build, we can sort it in order to have the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2joYyOoNe0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.fillna(' ')\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_words(data['message'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMGbj0Qxq7-",
        "colab_type": "text"
      },
      "source": [
        "Since we are facing some trouble when we want to display our plot, we build a little function to force colab to authorize plotly to show in the active cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAsH3yf0MuyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enable_plotly_in_cell()\n",
        "df1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
        "df1.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review after removing stop words')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUy8jkyyx5iG",
        "colab_type": "text"
      },
      "source": [
        "Of course here we see that the most common words are english and related to IT. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WuAwUlb62BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['review_len'] = data['message'].astype(str).apply(len)\n",
        "data['word_count'] = data['message'].apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXFdxpTyJv7",
        "colab_type": "text"
      },
      "source": [
        "Analysing the distribution of the length of the answers according if its best answer or not can also be interesting for us. In fact we can expect, that long and detailed answers are more likely to be labelled as best answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKUwYwP_75uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y0 = data.loc[data['is_best_answer'] == 1]['review_len']\n",
        "y1 = data.loc[data['is_best_answer'] == 0]['review_len']\n",
        "\n",
        "\n",
        "trace0 = go.Box(\n",
        "    y=y0,\n",
        "    name = 'Best answers',\n",
        "    marker = dict(\n",
        "        color = 'rgb(214, 12, 140)',\n",
        "    )\n",
        ")\n",
        "trace1 = go.Box(\n",
        "    y=y1,\n",
        "    name = 'Others',\n",
        "    marker = dict(\n",
        "        color = 'rgb(0, 128, 128)',\n",
        "    )\n",
        ")\n",
        "data_plot= [trace0, trace1]\n",
        "layout = go.Layout(\n",
        "    title = \"Review length Boxplot of best answers\"\n",
        ")\n",
        "enable_plotly_in_cell()\n",
        "fig = go.Figure(data=data_plot,layout=layout)\n",
        "iplot(fig, filename = \"Review Length Boxplot of best answers\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-8r2zkyj7m",
        "colab_type": "text"
      },
      "source": [
        "The results show that the length does not seem to have a significance impact on the label. In fact both distribution for best answers and other answers are really similar.\n",
        "\n",
        "In order to prevent memory issue when will do the embedding on our message, we check the length of the messages. In fact, a message that is too long can lead to a lack of RAM and can shut down our google colab session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtNqhaZ9x2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enable_plotly_in_cell()\n",
        "data['review_len'].iplot(\n",
        "    kind='hist',\n",
        "    bins=100,\n",
        "    xTitle='review length',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    title='Review Text Length Distribution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1PG5J60-AI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enable_plotly_in_cell()\n",
        "data['word_count'].iplot(\n",
        "    kind='hist',\n",
        "    bins=100,\n",
        "    xTitle='word count',\n",
        "    linecolor='black',\n",
        "    yTitle='count',\n",
        "    title='Review Text Word Count Distribution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsM62WbUMFsC",
        "colab_type": "text"
      },
      "source": [
        "# Model training with Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSi46AaS4W9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"data_cleaned.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvbDxbmXUyrK",
        "colab_type": "text"
      },
      "source": [
        "##Formatting data into Fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyLH6olCuV-u",
        "colab_type": "text"
      },
      "source": [
        "In order to train our model, we have to put our data in the Fast Text format. Fast Text is an open-source, free, lightweight library that allows users to learn text representation and text classifiers. Here, we will not train our model through Fast Text. We will build our model with the library Flair (presented below). Since Flair is using Fast Text corpuses, we need to adopt this format. \n",
        "\n",
        "![Fast text](https://fasttext.cc/img/ogimage.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxomhcZ_UwWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"data_cleaned.csv\")\n",
        "\n",
        "data[\"is_best_answer\"] = data[\"is_best_answer\"].replace(1, \"best\")\n",
        "data[\"is_best_answer\"] = data[\"is_best_answer\"].replace(0, \"not_best\")\n",
        "\n",
        "\n",
        "data = data[['is_best_answer', 'message']].rename(columns={\"is_best_answer\":\"label\", \"message\":\"text\"})\n",
        "\n",
        "# Cuda issues\n",
        "data = data[1:10000]\n",
        "\n",
        "data['label'] = '__label__' + data['label'].astype(str)\n",
        "data.iloc[0:int(len(data)*0.8)].to_csv('data_flair/train.csv', sep='\\t', index = False, header = False)\n",
        "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('data_flair/test.csv', sep='\\t', index = False, header = False)\n",
        "data.iloc[int(len(data)*0.9):].to_csv('data_flair/dev.csv', sep='\\t', index = False, header = False);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5lmEQNnwAgz",
        "colab_type": "text"
      },
      "source": [
        "A fast text corpus is defined by a train, a test and a dev dataset. To reduce the computing time and to avoid memory issues, we only usethe first 10 000 raws of our dataset for the moment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZuvQHkGYKck",
        "colab_type": "text"
      },
      "source": [
        "## Using Flair with gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCbJdvmNroCL",
        "colab_type": "text"
      },
      "source": [
        "Flair is python library created by Zalando research team. This library draws a very precise state of the art in NLP and permits to reach good score on regular problems.\n",
        "\n",
        "\n",
        "*   **A powerful NLP library** : Flair allows us to apply the state-of-the-art natural language processing to our text\n",
        "*   **Multilingual** : Flair supports multilingual text. This is very useful in our case since the data are multilingual\n",
        "* **A text embedding library** : Flair allows us to use and combine different embeddings. In this project we used Flair embedding combined with BERT embedding.\n",
        "* **A Pytorch NLP Framework** : the framework is directly build on Pytorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBxGDhJwzSul",
        "colab_type": "text"
      },
      "source": [
        "![Flair](https://avatars0.githubusercontent.com/u/30869512?s=400&v=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUsZI3DeuD5W",
        "colab_type": "text"
      },
      "source": [
        "You can find the Git-hub link to Flair library [here](https://github.com/zalandoresearch/flair)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkbiLZELtgt2",
        "colab_type": "text"
      },
      "source": [
        "Since Flair is a really specific library, it is not by default installed on our Virtual Machine provided by Google Colab. That is why we have to install it each we connect to our virtual machine : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdgCHuXTGqso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epwzQV_ZtsLC",
        "colab_type": "text"
      },
      "source": [
        "Once it is installed, we can load the function we will need in our project : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mNX6DyibgrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From flair\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings, BertEmbeddings, StackedEmbeddings, DocumentPoolEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer \n",
        "from flair.hyperparameter.param_selection import SearchSpace, Parameter\n",
        "from flair.visual.training_curves import Plotter\n",
        "from flair.hyperparameter.param_selection import TextClassifierParamSelector, OptimizationValue\n",
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "#from flair.data import TaggedCorpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pYim6VmtzIk",
        "colab_type": "text"
      },
      "source": [
        "Once flair is installed, we check that our device is well settled. We specify here that we want our tensorflow backend to use the GPU provided by Google Colab.![](https://cdn-images-1.medium.com/max/1200/0*c0dbic0TZRh9ILrx.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuvoAROCNfkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbu4_bqXN9uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import flair\n",
        "flair.device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieC_0cztYYpN",
        "colab_type": "text"
      },
      "source": [
        "## Importing Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkIwyBsuwZmw",
        "colab_type": "text"
      },
      "source": [
        "Since the device is now settled, we can start by importing our Corpus here. Flair is only using Fats text corpus, that is why the previous step of formatting our data was crucial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa3S992SpCW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change working directory\n",
        "%cd \"/content/drive/My Drive/Q & A/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HC6FGdmO_-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use your own data path\n",
        "data_folder = Path('data_flair')\n",
        "\n",
        "# load corpus containing training, test and dev data\n",
        "TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
        "                                                                     test_file='test.csv',\n",
        "                                                                     train_file='train.csv',\n",
        "                                                                     dev_file='dev.csv',)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xR_zmHAwxky",
        "colab_type": "text"
      },
      "source": [
        "To avoid memory issues, we just check that every reponse is not too long. We also do a small analysis using some descriptive statistics on our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCehDY89CBuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_tokens = 512\n",
        "corpus._train = [x for x in corpus.train if len(x) < max_tokens]\n",
        "corpus._dev = [x for x in corpus.dev if len(x) < max_tokens]\n",
        "corpus._test = [x for x in corpus.test if len(x) < max_tokens]\n",
        "stats = corpus.obtain_statistics()\n",
        "print(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJfplpT3vz8A",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-cY1l5NxuWw",
        "colab_type": "text"
      },
      "source": [
        "Flair provides different type of embedding. \n",
        "\n",
        "\n",
        "*  **Classic Word Embeddings**\n",
        "*   **Character Embeddings**\n",
        "* **Byte Pair Embeddings**\n",
        "* **Stacked Embeddings**\n",
        "* **Flair Embeddings**\n",
        "* **Bert Embeddings**\n",
        "\n",
        "\n",
        "Here we will combine BERT and Flair embedding. We decided to use Document Embeddings in stead of Word Embedding. Word Embeddings give us one embedding for individual words unlike document embeddings give us embeddings for an entire text. \n",
        "\n",
        "\n",
        "Different methods are available for document Embeddings : \n",
        "\n",
        "\n",
        "*   **Pooling** : This method calculates a pooling operation over all word embeddings in a documents. Here we take the *mean* off all the words in the answer. We can also use the operations *min* or *max*.\n",
        "*   **RNN** : Flair also provides a method based on RNN. The default RNN is a GRU-type but we can also use LSTM.\n",
        "\n",
        "In our project we use a document embedding pooling with Flair Embeddings combined with BERT Embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xxcD-PZ3c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()\n",
        "\n",
        "\n",
        "# init Flair embeddings\n",
        "flair_forward_embedding = FlairEmbeddings('multi-forward-fast')\n",
        "flair_backward_embedding = FlairEmbeddings('multi-backward-fast')\n",
        "\n",
        "# init multilingual BERT\n",
        "bert_embedding = BertEmbeddings('bert-base-multilingual-cased') \n",
        "\n",
        "#init Glove embedding\n",
        "glove_embedding = WordEmbeddings('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUqykTSqjbKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now create the StackedEmbedding object that combines all embeddings\n",
        "document_embeddings = DocumentPoolEmbeddings(\n",
        "    embeddings=[flair_forward_embedding,\n",
        "                flair_backward_embedding,\n",
        "                bert_embedding\n",
        "               ])\n",
        "\n",
        "#embedding gloobe\n",
        "document_embeddings_glove = DocumentPoolEmbeddings(embeddings = [glove_embedding])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JDb9x1Y3skm",
        "colab_type": "text"
      },
      "source": [
        "Once our Embedding is settled,  we can use the Textclassifier built by Flair : [text_classification_model.py](https://github.com/zalandoresearch/flair/blob/master/flair/models/text_classification_model.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p10zbIOgQgyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)\n",
        "\n",
        "# 6. initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmOfOYSwoSpL",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnXSrTTLGwLw",
        "colab_type": "text"
      },
      "source": [
        "Once our document embedding is done we can train the model. We have to choose the parameters of our models : \n",
        "\n",
        "\n",
        "\n",
        "*   **Learning rate** : The amount of change to the model during each step of this search process, or the step size. The learning rate controls how quickly or slowly a neural network model learns a problem. To set up the learning rate we will use the function find_learning_rate provided by flair.\n",
        "*   **Mini batch size** :  batch size  the number of samples to work through before updating the internal model parameters\n",
        "*  **Hidden size** : number of hidden layer in the LSTM\n",
        "*  **Maximum of epoch** : epoch defines the number times that the learning algorithm will work through the entire training dataset\n",
        "*  **Embeddings** : we can train our model with different embeddings\n",
        "*  **RNN Layers** : number of layers in our Recurrent Neural Network\n",
        "* **Dropout** :  Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network. This has the effect of reducing overfitting and improving model performance.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhW73locbyvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8. start the training\n",
        "trainer.train(\"data_flair\",\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=8,\n",
        "              embeddings_in_memory = False,\n",
        "              max_epochs=150, \n",
        "              checkpoint = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S8g97_SOlnH",
        "colab_type": "text"
      },
      "source": [
        "We are facing an issu with Google Colab. In fact, Google Colab close the connection to the virtual machine to fight against coder who are using google colab to mine bitcoin. To solve this problem, we save the model after each epoch with the parameter *checkpoint = True*. Thus, even if we are disconnected from google colab we can still resume the training after. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6PEkuuTG6gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = ModelTrainer.load_from_checkpoint(Path(\"data_flair/checkpoint.pt\"), 'TextClassifier', corpus)\n",
        "\n",
        "trainer.train(\"data_flair\",\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=8,\n",
        "              embeddings_in_memory = False,\n",
        "              max_epochs=150,\n",
        "              checkpoint=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAtOHwc9npZq",
        "colab_type": "text"
      },
      "source": [
        "## Loading best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGTqT7HKR_2Z",
        "colab_type": "text"
      },
      "source": [
        "Once our model is trained we can load the best model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH6fLLScnoxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = ModelTrainer.load_from_checkpoint(Path(\"data_flair/best-model.pt\"), 'TextClassifier', corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheLlS79dUds",
        "colab_type": "text"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnZ8bIZ8Sq8n",
        "colab_type": "text"
      },
      "source": [
        "To define the best learning rate, we can use this fuction provided by NLP. This graph represents the loss for different learning rate. We have here to choose the learning rate which maximises the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWSZKSD6bwLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 7. find learning rate\n",
        "learning_rate_tsv = trainer.find_learning_rate('data_flair',\n",
        "                                               'learning_rate.tsv',\n",
        "                                              mini_batch_size=32)\n",
        "\n",
        "# 8. plot the learning rate finder curve\n",
        "plotter = Plotter()\n",
        "plotter.plot_learning_rate(learning_rate_tsv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmcGHkN83xZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image('data_flair/learning_rate.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IEMEs-MdaiI",
        "colab_type": "text"
      },
      "source": [
        "## Hyper parameters selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_7BsQUjS6bz",
        "colab_type": "text"
      },
      "source": [
        "In order to select the best parameters for our model, Flair also provides an optimizer. This optimizer his highly time consuming but can be really convenient to find the best parameters for our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4FD3B7JdYgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define your search space\n",
        "search_space = SearchSpace()\n",
        "search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[\n",
        "  [WordEmbeddings('glove')],\n",
        "   [FlairEmbeddings('multi-forward-fast'), FlairEmbeddings('multi-backward-fast')]\n",
        "])\n",
        "search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[32, 64, 128])\n",
        "search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])\n",
        "search_space.add(Parameter.DROPOUT, hp.uniform, low=0.0, high=0.5)\n",
        "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[0.05, 0.1, 0.15, 0.2])\n",
        "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[8, 16, 32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8qjKiecdpYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the parameter selector\n",
        "param_selector = TextClassifierParamSelector(\n",
        "    corpus, \n",
        "    False, \n",
        "    'data_flair', \n",
        "    'lstm',\n",
        "    mini_batch_size=1,\n",
        "    max_epochs=150, \n",
        "    training_runs=3,\n",
        "    optimization_value=OptimizationValue.DEV_SCORE\n",
        ")\n",
        "\n",
        "# start the optimization\n",
        "param_selector.optimize(search_space, max_evals=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyhmtw81Smr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8. plot training curves (optional)\n",
        "from flair.visual.training_curves import Plotter\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves('data_flair/loss.tsv')\n",
        "plotter.plot_weights('data_flair/weights.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOKYpzwufsZM",
        "colab_type": "text"
      },
      "source": [
        "# END------------------------"
      ]
    }
  ]
}